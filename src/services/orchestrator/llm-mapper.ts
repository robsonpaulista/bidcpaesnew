// ==========================================
// MAPEADOR LLM - Pergunta ‚Üí Inten√ß√£o + Entidades
// ==========================================
// IMPORTANTE: LLM N√ÉO DECIDE NADA, apenas mapeia e extrai entidades
// O orquestrador (c√≥digo) decide o plano, queries, l√≥gica

import { BusinessIntention, intentions } from './intentions'

// ==========================================
// TIPOS
// ==========================================

export interface LLMMappingResult {
  intent: BusinessIntention
  confidence: number
  entities: {
    kpi?: string
    produto?: string
    periodo?: string
    linha?: string
    area?: string
    fornecedor?: string
    [key: string]: string | undefined
  }
}

// ==========================================
// CONFIGURA√á√ÉO
// ==========================================
// IMPORTANTE: Em produ√ß√£o, esta fun√ß√£o deve rodar no BACKEND
// No frontend, apenas chama a API /api/orchestrator/ask

interface LLMConfig {
  provider: 'groq' | 'huggingface' | 'gemini' | 'local'
  apiKey?: string
  model?: string
}

// Detecta se est√° rodando no backend (Node.js) ou frontend (Vite)
// No Vercel Serverless Functions, process.env existe e import.meta.env n√£o
const isBackend = typeof process !== 'undefined' && typeof process.env !== 'undefined' && process.env.GROQ_API_KEY

const config: LLMConfig = {
  provider: isBackend
    ? (process.env.LLM_PROVIDER as LLMConfig['provider']) || 'local'
    : 'local', // Frontend sempre usa 'local' (chama API)
  apiKey: isBackend
    ? process.env.GROQ_API_KEY || process.env.LLM_API_KEY
    : undefined, // Frontend NUNCA deve ter a key
  model: isBackend
    ? process.env.LLM_MODEL || 'llama-3.1-8b-instant'
    : 'llama-3.1-8b-instant' // Frontend n√£o usa, mas precisa de valor padr√£o
}

// ==========================================
// PROMPT PARA MAPEAMENTO
// ==========================================
// LLM retorna APENAS inten√ß√£o + entidades (JSON estruturado)

function createMappingPrompt(question: string): string {
  const intentionsList = Object.entries(intentions)
    .map(([id, def]) => `- ${id}: ${def.name} (${def.description})`)
    .join('\n')

  return `Voc√™ √© um assistente que mapeia perguntas de neg√≥cio para inten√ß√µes e extrai entidades.

Pergunta: "${question}"

Inten√ß√µes dispon√≠veis:
${intentionsList}

EXEMPLOS DE MAPEAMENTO:
- "faturamento mensal", "receita", "evolu√ß√£o anual", "melhor/pior m√™s", "sazonalidade do faturamento" ‚Üí analyze_revenue_trend
- "sazonalidade nas compras", "sazonalidade de mat√©rias-primas", "padr√£o de compras" ‚Üí analyze_supplier_performance
- "margem", "lucro", "custo", "queda de margem" ‚Üí analyze_margin_decline
- "perdas", "refugo", "desperd√≠cio" ‚Üí analyze_losses
- "oee", "efici√™ncia de produ√ß√£o" ‚Üí analyze_production_efficiency
- "mix de produtos", "mix de vendas" ‚Üí analyze_sales_mix
- "rota", "custo por rota", "efici√™ncia de rotas" ‚Üí analyze_logistics_cost

Sua tarefa:
1. Identificar a inten√ß√£o mais adequada (seja espec√≠fico, n√£o gen√©rico)
2. Extrair entidades mencionadas (kpi, produto, per√≠odo, linha, √°rea, fornecedor)

IMPORTANTE:
- Se a pergunta menciona "faturamento", "receita", "evolu√ß√£o", "melhor m√™s", "pior m√™s", "sazonalidade do faturamento" ‚Üí use analyze_revenue_trend
- Se a pergunta menciona "sazonalidade" + "compra"/"mat√©ria-prima"/"fornecedor" ‚Üí use analyze_supplier_performance
- Se a pergunta menciona "margem", "lucro", "custo" ‚Üí use analyze_margin_decline
- Se a pergunta menciona "rota", "custo por rota", "efici√™ncia" + "log√≠stica" ‚Üí use analyze_logistics_cost
- Seja preciso: escolha a inten√ß√£o que melhor descreve o objetivo da pergunta. Considere o CONTEXTO (compra vs faturamento vs log√≠stica)

Retorne APENAS um JSON v√°lido no formato:
{
  "intent": "ID_DA_INTENCAO",
  "confidence": 0.0-1.0,
  "entities": {
    "kpi": "nome_do_kpi_se_mencionado",
    "produto": "nome_do_produto_se_mencionado",
    "periodo": "per√≠odo_se_mencionado",
    "linha": "linha_se_mencionada",
    "area": "√°rea_se_mencionada",
    "fornecedor": "fornecedor_se_mencionado"
  }
}

IMPORTANTE: Retorne APENAS o JSON, sem texto adicional.`
}

// ==========================================
// GROQ API
// ==========================================

async function mapWithGroq(question: string): Promise<LLMMappingResult> {
  if (!config.apiKey) {
    const envHint = isBackend 
      ? 'GROQ_API_KEY ou LLM_API_KEY no Vercel Environment Variables'
      : 'Esta fun√ß√£o deve rodar no backend. Chame /api/orchestrator/ask'
    
    if (import.meta.env?.DEV || process.env.NODE_ENV === 'development') {
      console.error('‚ùå Groq API key n√£o configurada')
      console.log(`üí° Configure ${envHint}`)
    }
    throw new Error('Groq API key n√£o configurada')
  }

  // Log da requisi√ß√£o (apenas em dev, sem expor a key)
  const isDev = import.meta.env?.DEV || process.env.NODE_ENV === 'development'
  if (isDev) {
    console.log('üåê Enviando requisi√ß√£o para Groq API...', {
      model: config.model,
      endpoint: 'https://api.groq.com/openai/v1/chat/completions',
      hasApiKey: !!config.apiKey
    })
  }

  // Timeout de 3 segundos
  const controller = new AbortController()
  const timeoutId = setTimeout(() => controller.abort(), 3000)

  try {
    const response = await fetch('https://api.groq.com/openai/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${config.apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: config.model || 'llama-3.1-8b-instant',
        messages: [
          {
            role: 'system',
            content: 'Voc√™ √© um assistente que mapeia perguntas para inten√ß√µes e extrai entidades. Retorne APENAS JSON v√°lido.'
          },
          {
            role: 'user',
            content: createMappingPrompt(question)
          }
        ],
        temperature: 0.1,
        response_format: { type: 'json_object' }, // For√ßa resposta JSON
        max_tokens: 200
      }),
      signal: controller.signal
    })

    clearTimeout(timeoutId)

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}))
      const errorMessage = errorData.error?.message || response.statusText
      if (isDev) {
        console.error('‚ùå Erro na API Groq:', {
          status: response.status,
          statusText: response.statusText,
          error: errorMessage
        })
      }
      throw new Error(`Groq API error (${response.status}): ${errorMessage}`)
    }

    const data = await response.json()
    const content = data.choices[0]?.message?.content?.trim()
    
    if (!content) {
      if (isDev) {
        console.error('‚ùå Resposta vazia do Groq:', data)
      }
      throw new Error('Resposta vazia do Groq')
    }

    if (isDev) {
      console.log('üì• Resposta bruta do Groq:', content.substring(0, 200) + (content.length > 200 ? '...' : ''))
    }

    return parseLLMResponse(content)
  } catch (error) {
    clearTimeout(timeoutId)
    
    if (error instanceof Error && error.name === 'AbortError') {
      throw new Error('Timeout ao chamar Groq API (3s)')
    }
    throw error
  }
}

// ==========================================
// GOOGLE GEMINI
// ==========================================

async function mapWithGemini(question: string): Promise<LLMMappingResult> {
  if (!config.apiKey) {
    throw new Error('Gemini API key n√£o configurada')
  }

  const response = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key=${config.apiKey}`,
    {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        contents: [{
          parts: [{
            text: createMappingPrompt(question)
          }]
        }],
        generationConfig: {
          temperature: 0.1,
          responseMimeType: 'application/json'
        }
      })
    }
  )

  if (!response.ok) {
    throw new Error(`Gemini API error: ${response.statusText}`)
  }

  const data = await response.json()
  const content = data.candidates?.[0]?.content?.parts?.[0]?.text?.trim()
  
  if (!content) {
    throw new Error('Resposta vazia do Gemini')
  }

  return parseLLMResponse(content)
}

// ==========================================
// PARSER DE RESPOSTA LLM
// ==========================================
// Extrai JSON da resposta (pode ter texto antes/depois)

function parseLLMResponse(content: string): LLMMappingResult {
  let parsed: any
  
  // CORRE√á√ÉO CR√çTICA: Tenta JSON.parse direto primeiro (j√° for√ßa JSON com response_format)
  try {
    parsed = JSON.parse(content)
  } catch {
    // Fallback: tenta extrair JSON com regex (√∫ltimo recurso)
    const jsonMatch = content.match(/\{[\s\S]*\}/)
    if (!jsonMatch) {
      throw new Error('JSON n√£o encontrado na resposta')
    }
    try {
      parsed = JSON.parse(jsonMatch[0])
    } catch (parseError) {
      const errorMessage = parseError instanceof Error ? parseError.message : String(parseError)
      throw new Error(`Erro ao parsear JSON: ${errorMessage}`)
    }
  }
  
  // Valida estrutura b√°sica
  if (!parsed || typeof parsed !== 'object') {
    throw new Error('Resposta n√£o √© um objeto JSON v√°lido')
  }
  
  // Valida intent
  if (!parsed.intent || typeof parsed.intent !== 'string') {
    throw new Error('Campo "intent" inv√°lido ou ausente')
  }

  // Valida se inten√ß√£o existe
  if (!(parsed.intent in intentions)) {
    throw new Error(`Inten√ß√£o "${parsed.intent}" n√£o existe`)
  }

  // CORRE√á√ÉO CR√çTICA: Usa nullish coalescing (??) em vez de || para confidence = 0
  const rawC = parsed.confidence ?? 0.8
  const c = Number(rawC)
  const confidence = Number.isFinite(c) 
    ? Math.min(1, Math.max(0, c))
    : 0.8

  // Valida entities
  const entities = (typeof parsed.entities === 'object' && parsed.entities !== null && !Array.isArray(parsed.entities))
    ? parsed.entities
    : {}

  return {
    intent: parsed.intent as BusinessIntention,
    confidence,
    entities
  }
}

// ==========================================
// FALLBACK: Mapeamento por Keywords
// ==========================================
// Sempre funciona, mesmo sem LLM

function mapWithKeywords(question: string, context?: Record<string, unknown>): LLMMappingResult {
  const lowerQuestion = question.toLowerCase()
  const intentionScores: Array<{ intention: BusinessIntention; score: number }> = []

  for (const [intentionId, definition] of Object.entries(intentions)) {
    let score = 0
    
    for (const keyword of definition.keywords) {
      if (lowerQuestion.includes(keyword)) {
        score += 1
      }
    }
    
    if (context?.area) {
      const areaAgents = definition.agents
      if (context.area === 'compras' && areaAgents.includes('compras_fornecedores')) score += 2
      if (context.area === 'producao' && areaAgents.includes('producao')) score += 2
      if (context.area === 'estoque' && areaAgents.includes('estoque_logistica')) score += 2
      if (context.area === 'comercial' && areaAgents.includes('comercial')) score += 2
      if (context.area === 'financeiro' && areaAgents.includes('financeiro')) score += 2
    }
    
    intentionScores.push({ intention: intentionId as BusinessIntention, score })
  }

  intentionScores.sort((a, b) => b.score - a.score)
  const selectedIntention = intentionScores[0].score > 0 
    ? intentionScores[0].intention 
    : 'general_overview'

  // Extrai entidades b√°sicas por keywords
  const entities: LLMMappingResult['entities'] = {}
  
  if (lowerQuestion.includes('margem')) entities.kpi = 'margem'
  if (lowerQuestion.includes('oee')) entities.kpi = 'oee'
  if (lowerQuestion.includes('otif')) entities.kpi = 'otif'
  if (lowerQuestion.includes('acur√°cia') || lowerQuestion.includes('acuracia')) entities.kpi = 'acuracia'
  
  if (lowerQuestion.includes('floc√£o') || lowerQuestion.includes('flocao')) entities.produto = 'floc√£o'
  if (lowerQuestion.includes('farinha')) entities.produto = 'farinha'
  if (lowerQuestion.includes('p√£o franc√™s')) entities.produto = 'P√£o Franc√™s'
  
  if (lowerQuestion.includes('dezembro')) entities.periodo = 'dezembro'
  if (lowerQuestion.includes('novembro')) entities.periodo = 'novembro'
  
  if (lowerQuestion.includes('linha 1')) entities.linha = 'Linha 1 - Franc√™s'
  if (lowerQuestion.includes('linha 2')) entities.linha = 'Linha 2 - Forma'
  if (lowerQuestion.includes('linha 3')) entities.linha = 'Linha 3 - Doces'

  return {
    intent: selectedIntention,
    confidence: Math.min(0.9, 0.5 + (intentionScores[0].score * 0.1)),
    entities
  }
}

// ==========================================
// CACHE DO LLM MAPPER (PERSISTENTE)
// ==========================================
// Usa Redis (Upstash) em produ√ß√£o, fallback em mem√≥ria

import { getCachedMapping, setCachedMapping } from './cache'

// ==========================================
// FUN√á√ÉO PRINCIPAL
// ==========================================
// LLM apenas mapeia, orquestrador decide o resto

export async function mapQuestionToIntentionWithLLM(
  question: string,
  context?: Record<string, unknown>
): Promise<LLMMappingResult> {
  // Log de debug (apenas em desenvolvimento)
  if (import.meta.env.DEV) {
    console.log('üîç Mapeando pergunta com LLM:', {
      provider: config.provider,
      hasApiKey: !!config.apiKey,
      question: question.substring(0, 50) + '...'
    })
  }

  // Verifica cache primeiro (Redis ou mem√≥ria)
  const cached = await getCachedMapping(question, context)
  if (cached) {
    if (import.meta.env?.DEV || process.env.NODE_ENV === 'development') {
      console.log('üíæ Cache hit:', question.substring(0, 50))
    }
    return cached
  }

  // Se n√£o h√° API key ou provider √© 'local', usa fallback
  if (!config.apiKey || config.provider === 'local') {
    if (import.meta.env.DEV) {
      console.log('‚ö†Ô∏è Usando fallback (keywords) - LLM n√£o configurado')
    }
    const result = mapWithKeywords(question, context)
    // Salva no cache mesmo para fallback
    await setCachedMapping(question, context, result)
    return result
  }

  try {
    let result: LLMMappingResult

    switch (config.provider) {
      case 'groq':
        if (import.meta.env.DEV) {
          console.log('üöÄ Usando Groq para mapeamento...')
        }
        result = await mapWithGroq(question)
        if (import.meta.env.DEV) {
          console.log('‚úÖ Groq mapeou:', {
            intent: result.intent,
            confidence: result.confidence,
            entities: result.entities
          })
        }
        break
      case 'gemini':
        if (import.meta.env.DEV) {
          console.log('üöÄ Usando Gemini para mapeamento...')
        }
        result = await mapWithGemini(question)
        break
      default:
        if (import.meta.env.DEV) {
          console.log('‚ö†Ô∏è Provider n√£o suportado, usando fallback')
        }
        return mapWithKeywords(question, context)
    }

    // Merge entidades do contexto (prioridade)
    if (context) {
      const contextEntities: Record<string, string | undefined> = {}
      for (const [key, value] of Object.entries(context)) {
        contextEntities[key] = typeof value === 'string' ? value : String(value ?? '')
      }
      result.entities = { ...result.entities, ...contextEntities }
    }

    // Salva no cache (Redis ou mem√≥ria)
    await setCachedMapping(question, context, result)

    return result
  } catch (error) {
    console.warn('‚ùå Erro no mapeamento LLM, usando fallback:', error)
    // Sempre tem fallback
    const fallbackResult = mapWithKeywords(question, context)
    // Salva fallback no cache tamb√©m
    await setCachedMapping(question, context, fallbackResult)
    return fallbackResult
  }
}

export { config as llmConfig }

